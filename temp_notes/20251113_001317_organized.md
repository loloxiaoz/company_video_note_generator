# 超越RAG的agentic search :从“被动投喂”到“主动探索”-Anthropic《AI Agent的高效上下文工程》③ - 整理版

## 视频信息
- 作者：慢学AI
- 时长：269.566秒
- 平台：bilibili
- 链接：https://www.bilibili.com/video/BV1gJxVzMEfW/?spm_id_from=333.337.search-card.all.click&vd_source=2237a6a04aa93d29745585f9dbc923fe

## 内容整理

# 你的AI助手为什么总是"失忆"？动态上下文才是让它真正聪明的秘密

你有没有遇到过这样的情况：向AI提问时，它给出的答案要么信息过时，要么答非所问，甚至在复杂任务中反复遗忘你刚才说过的话？问题的根源在于，大多数AI系统只是被动地接收信息，而不会主动寻找和更新所需的知识。

这就像你雇了一个助手，但他只能依靠入职培训时学到的内容工作，既不会查资料，也不会根据实际情况调整策略。想象一下，这样的助手能完成多少复杂任务？

今天我们要聊的，就是如何让AI从"死记硬背"进化到"灵活应变"——这个关键技术叫做**动态上下文**。如果说之前的静态上下文是给AI造了一个大脑，那么动态上下文就是让这个大脑开始感知和探索真实世界。

## 封闭系统的困境

让我们先理解问题的本质。如果一个AI系统只能依赖初始设置（也就是我们说的"出场设置"）来工作，它本质上就是一个封闭系统。但现实世界的任务远比这复杂得多——信息会更新，需求会变化，任务会演进。

这就引出了一个核心挑战：**如何将海量的外部信息，高效又精准地注入到AI有限的上下文环境里？**

想象一下，你的大脑在某一时刻只能处理有限的信息量，但世界上的知识却是无穷无尽的。人类通过什么方式解决这个矛盾？我们会根据需要去查资料、问专家、做实验——这就是动态获取信息的过程。AI也需要类似的能力。

## 图书馆管理员模式：推理前检索

第一种解决方案叫做"推理前检索"（Retrieval-Augmented Generation，简称RAG），它的工作流程就像图书馆管理员帮你找书。

当你提出一个问题时，系统会先把这个问题转换成数学语言（向量），然后在向量数据库中搜索最相似的文档片段，挑出最相关的几条内容，拼接到上下文里，最后交给大模型生成答案。整个过程是一条单向的流水线：在模型开始思考之前，所有资料就已经准备好了。

这种方式的优势很明显：**稳定、快速、成本低**。一次检索，一次调用，适合处理知识问答、客服、固定流程等任务。

但它也存在三个明显的问题：

首先，**信息可能过时**。就像图书馆里的旧书，检索到的内容可能已经不是最新版本。

其次，**检索不够智能**。系统只是机械地匹配语义相似度，容易拿到方向偏差的内容，反而污染了上下文。比如你问"如何优化Python代码性能"，它可能检索到"如何优化JavaScript代码性能"的文档，看起来相关，实际上帮不上忙。

第三，**无法探索**。一旦初始检索方向错误，模型就被锁死在错误的结果里，只能被动等待信息投喂，没有纠正的机会。

## 侦探模式：及时检索

第二种方案叫做"及时检索"（Agentic Search），它彻底改变了游戏规则。在这种方式中，模型不再被动，而是像侦探一样主动探索。

当你提出问题后，模型会先思考："我需要调用什么工具？"它可能先执行`List files`查看有哪些文件，根据返回结果选择打开其中一个，读取前几行，然后观察新信息，再决定下一步是搜索别的路径，还是直接生成答案。

整个过程是**循环动态**的。上下文的构建不再是推理前的一次性准备，而是推理过程中的动态迭代。就像侦探拿到案子后，自己去档案室查目录、看笔记、比对证据，每发现一个线索就更新推理方向。

这种方式带来三个关键优势：

**信息永远是最新的**。因为每一步都是及时调用工具，不存在信息过期的问题。

**信噪比高**。只取当下真正需要的信息，不会被无关内容淹没。想象一下，如果侦探每次都要翻阅整个档案室的所有文件，效率会有多低？

**具备探索能力**。遇到模糊问题时可以边查边想、边想边查，对于复杂的多步推理任务尤其关键。

## 智能探索的两个秘密武器

那么，AI是如何做到智能探索的？这里有两个关键机制。

第一个是**元数据**。AI不仅看内容本身，还能理解内容的信息结构。比如，一个叫`test-utils.py`的文件放在`test`文件夹里，这就表明该文件是用于测试的。数据库表名、网页链接地址、文件更新时间等，都能成为它判断方向的信号。就像你看到文件夹名、时间戳、路径结构，会自然猜测其作用一样。

第二个机制是**渐进式披露**。这个概念借鉴自用户体验设计，意思是不要一次性把所有信息都塞给模型，而是让它自己逐步发现，一层一层揭开需要的上下文。这样模型能始终聚焦在最相关的任务子集上，不会因信息过多而分心。可以理解为，它学会了按需思考。

## 最优解：混合策略

那么这两种方式哪个更好呢？答案是：**各有取舍，真正的最优解是混合使用**。

推理前检索（RAG）的优势是速度快、结构简单，适合知识问答、客服、固定流程任务。及时检索（Agentic Search）的优势是灵活、能探索未知领域，更适合研究、编程、分析类任务。

在真正复杂的产品里，聪明的做法是结合两者的优势。以Claude的代码助手为例：启动时会预先加载一份`claude.md`文件，让模型对整个项目有初步理解——这部分属于RAG。同时，它也配备了像`GlobSearch`这样的工具，能在运行时及时检索文件、分析数据——这部分就是Agentic Search。这样既保留了速度，又具备了探索能力。

## 从感知到记忆

我们已经帮AI装上了静态大脑（出场设置），又赋予了它动态感官（信息检索）。它不仅能想，还能找、能看、能探索。

但故事还没结束。想象一个场景：你让AI帮你完成一个持续数小时甚至几天的复杂项目，比如分析一个大型代码库并提出重构方案。在这个过程中，AI会做出许多关键决策，发现许多重要线索。但如果它的"记忆"容量有限，就会在长时间任务中遗忘之前的关键信息，导致决策前后矛盾，甚至重复劳动。

这就引出了下一个问题：**当任务持续很长时间时，AI该怎么记住过去，避免在长期任务中遗忘关键决策呢？**

这就是我们下一期要探讨的内容：如何为长期任务构建上下文？我们会介绍三种高级架构，让AI真正具备持续思考的能力——就像人类的长期记忆一样，既能回顾历史，又能规划未来。

如果你想让自己的AI助手从"健忘的新手"进化成"经验丰富的专家"，记得关注下一期内容。