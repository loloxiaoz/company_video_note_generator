# 算法视角深度解读 Kimi K2 和 K2 Thinking，从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据

## 视频信息
- 作者：chaofa用代码打点酱油
- 时长：1960.833秒
- 平台：bilibili
- 链接：https://www.bilibili.com/video/BV1yikRBvEwy/?spm_id_from=333.40138.feed-card.all.click&vd_source=2237a6a04aa93d29745585f9dbc923fe

## 原始转录内容

最近KimiK2 thinking火了,内网外网都说很牛,能写完整的应用。但是没有人告诉你它是怎么训练出来的,最重要的是这些训练方法我们能不能在自己的业务中用呢?我看了一圈啊,大家要不是在赛benchmark,要不是感叹它太牛了,但是作为一个大模型的从业者,我更关心的是这些能力是怎么训练出来的,我能不能借鉴这些方法,我在自己的工作中能不能提升我的业务价值,我是潮发。本期视频,我就深入解读一下KimiK2的技术报告,提炼出四个可以在业务中使用的技巧。从预训练的moonclip,到数据改写优化,然后到后训练的数据合成pipeline,以及self-adjudge机制,我会用比较接地气的一种机制来给大家讲一下它们是什么,为什么有用以及怎么用。如果你是在做大模型训练或者说想做模型的agentic能力,这些视频值得你看完,记得点赞收藏,我们开始吧。接下来就带大家解读一下KimiK2和K2Thinking的技术细节。我们可以想一下,我们第一手资料肯定是它发表的官方内容,比如说K2Thinking,但是K2Thinking基本上都是秀了一些肌肉,告诉你benchmark有多强,实际上技术细节不多。所以我们只能从K2里面去获取到一些内容,然后去了解它的技术细节,尤其是K2的话,它其实是有一篇paper的,然后这个paper里面也有很多的技术细节,然后很值得我们学习。但是作为一个创法工程师的话,我可能比较关注的是预训链和后训链相关的一些东西,然后比如说它这里面的一些RL infrastructure这种基建相关的一些东西,然后还有什么MP8训练啊,我可能关注的就没有那么多了,所以说我对于这个paper的整体结构去做了一个重新的梳理,让它变得更适合一些做训练的一些创法工程师学习的一些内容。然后主要包含的是预训链和后训链,然后以及我的一些思考,比如说一些核心启发,还有一些其他的东西,然后我们就来阅读那个blog,大家也可以自己去阅读那个原文,然后就在这个地方,大家都可以访问到。首先就是来讲一下那个K2 thinking到底哪里比较吸引我,它核心的那个比较吸引我的点主要是它的agentic能力,比如说它可以两三百次调用工具,就有点那个cloud的那种感觉了,然后尤其是外网比较吹的比较多的也都是说它的那个工具调用能力比较强,能够和那个cloud code用起来比较方便,但是我们的话主要还是学习,学习它是怎么训练出来的嘛,然后K2和K2 thinking是出同源嘛,所以只是说在那个K2的基础上加了那个thinking能力,因为它放出K2来的时候大家都知道它会出一个那个thinking模型嘛,这个是肯定知道的,所以说我们就来看一下它怎么训练出来的,然后要了解这个训练出来的我们主要就关心三个问题,第一个的话是它预训练怎么预训练的,然后怎么后训练的,然后以及怎么做那个tester time的scaling,这个其实上一篇文章也都讲过了,这个还是三月份的,我们就是去看一下这半年来有什么变化,然后以及业界的趋势有什么变化,我们首先来看一下K2和K2 thinking的一个整体结构啊,首先就来看一下这张图,可以看到左边跟右边基本上是一样的,也就是说Kimi K2它的结构已经跟那个deep sick R1基本上完全一模一样,只是说他们做了一些实验去做相关的一些改进,比如说那个attention的hide数量减少,然后R1它是128,然后K2的话是64,相当于attentionhide减少了,然后第二个比较重大的改进就是这个expert的数量增加了,你看从256增加到384,尤其是他们在paper里面还提了一个叫expert scaling law的一个东西啊,也就是说你expert数量增加的越多,然后它的效果越好,然后除此之外呢,比较重要的一个区别点就是你可以看到那个deep sick R1它是前三层都用的是FFN,然后这个K2呢它是只有第一层,它是只有第一层用了FFN,也就是说它比deep sick R1更激进的使用了MOE的一个架构,然后其他的比如说你的上下纹长度都是128K,然后还有一些其他的一些细节,比如说你的vocab size啊这种东西的话就大家自己去看一下,我个人觉得比较关键的就是两个点,一个是你的attention head数量减少了,然后第二个是expert数量增加了,然后以及它更激进的去做了那个MOE,然后我们就来了解一下这些模型的关系是什么,就是我们刚刚提到了deep sick R1,然后提到了K2 thinking,然后还有K2,我们可以看一下那个K2和K2 thinking的关系是什么,K2的话你可以理解为是一个基座模型,它的基座模型的话分为两个版本,第一个版本的话叫做K2 base,也就是说这些模型的话主要是用来给那些研究者或者说开发者,还有一些企业用户去对里面进行做微调,因为你想要完全自己丛林去做预训练的话相对来说还是比较麻烦的,然后第二个的话就是一个叫instruct模型,这里的话主要是大家可以直接使用了,可以直接在调API去业务里面使用,使用起来它是一个非推理的模型,然后K2 thinking就相当于是在那个K2 base上面去做Test time scaling,然后以及更强的工具调用能力的一个thinking模型,就有点类似于这个Kimi K1.5的这个thinking模型,就是一个long-COT的一个能力,就相当于它是一个具有更强工具能力调用的long-COT的一个模型,这就是K2 thinking,然后它的那个基础呢就是一个1T参数的,这个参数量真的很大,1T参数的,然后但是它的那个激活参数相对来说比较少,这主要是得益于它的那个expert数量比较多嘛,所以说它可以在激活比较少的情况下获得那个整体参数量比较大,也就是说它的智能是很高的,但是你在运行的过程中它又不至于特别慢,大概就是这样,所以你可以理解为K2 thinking和K2基本上就是同一个模型,只是说他们的那个后训练的数据有一些不同,但是预训练的数据基本上是一样的嘛,对吧,所以说我们就来看一下它预训练到底是怎么训练出来的呢,我们在看那个K2的时候,实际上我们发现以前的那些模型都很少有各种各样的一些结构啊,或者说优化器相关的改进,优化器你就用i到w嘛,但是对于K2来说比较惊喜的是,他们已经在大规模的去使用moon这个优化器了,然后同时他们提出了一个叫moon clip的一个优化方法,去提升那一个token的利用效率,然后我们可以解释一下,为什么我们要一个更好的优化器,就是我们首先来回答第一个问题,我们为什么要更好的优化器,首先是因为我们人类产生的那些文本或者说token它是有限的,那么你想要在有限的token里面去提升智能,那么你就只能更激进或者说更有效率的去提取那些文本里面的一些信息,所以说我们假设有一个系数叫做token利用率的一个系数,如果你这个优化器能够让你的token利用率越高,那么你这一个优化器就是越好的,然后从那个实验来看,就是这个moon优化器他们以前的一个工作叫moonlight,证明了在大规模的一个训练里面,然后在相同的配置里面,那个moon优化器它可以有一个更低的loss,那更低的loss就是说明你的ppl更低嘛,那么也就说明你能够从那些token里面学到的东西更多,因为你的困惑度降低了,对不对,你困惑度降低了就说明你学到的智能是更高的,那么所以说他们用了这个moon优化器,然后但是这个moon优化器也不是说非常完美的,就是在那个attention的计算的过程中,它相对于www来说,它是更容易的出现一个训练不稳定的问题,然后这个东西也有一些解决方法,比如说千问的话,它实际上就用了一个叫query和key的normalization的一个方法,然后主要是为了防止数值溢出,normalization就是相当于你在做那个成绩之前去给它做一次norm,然后这个kimi的话,他们不是用的这个方法,他们用的是一个叫做clip的方法,是直接去clipattention的logic,也就是说它不是在你计算attention score之前,而是在它之后,所以说它提出了这样一个方法,然后它的核心思路的话就是,我直接在你更新完一次attention前向和反向传播之后,然后我去对它的query和key做一个重新的举证的映射,其实也有点像normalization,只是说它这个方法不是normalization,而是说直接呈上一个系数来去做clip,这样应该可以理解吧,你normalization的话是你在你自己的weight里面去做normalization,就比如说规划是一种normalization,但是clip的话它是呈上了另外一个举证,然后你可以看到它的方法也相对来说比较简单,就是你每一个比如说head,假设这个qi是一个head的话,那么你对于这一个head来说你呈上某一个举证,那么然后这个举证的话它是带有一定缩放的功能的,那么它就是一个它就是能做到缩放的一个能力嘛,然后这个音字就是这个叫做,我也不知道这个东西读什么,反正可能读读eta,然后它的那个公式的话也就是你用你历史的,历史的所有的head里面的q和k的成绩,取出它的最大值,然后设计一个这个t,这里应该是tau,我可能打错了就是tau,这是超三数,然后取它的最小,然后再去呈上你的那个缩放音子,就是你原来的那个wait,就wq的wait呈上这么一个缩放音子,就相当于把你的原来比较高的数值,然后把它降低,这样子的话就做了一个就相当于做到了一个clip的一个效果嘛,然后这里面其实还有一些其他的细节啊,我这里没有讲,就比如说它每一个head都是设置了不同的这样一个缩放的音子,这是一些技术细节啊,这个大家可以去自己去看一下,然后从那个实验结果来看,它实现的最终效果是什么,就是它的那个loss相对来说是比较稳定的,是没有那种moe经常出现的这种loss突然激增的一个现象,你可以看到它的loss是非常稳定的,所以说这个moonclip这样一种方法的话,对于如果你去做预训练的话,它相对来说还是比较有用的,然后并且被证明了有效,然后除此之外,那么可能就是我们那些大模型应用算法工程师比较关注的,那就是他们的那些训练数据是怎么优化的,因为我们都知道训练数据语料是有限的,那你如果你去重复的去学习这部分的话,那有可能会过你盒嘛,你重复的学习的话是有可能会过你盒的,那么那个kimi的话,他们提出的一种方法就是做文本改写,然后它的核心思路就是,我们可以对一些相似的文本去做改写,那么它就变成了两条不同的query,但是他们的含义还是相对来说比较类似的,你可以不用去重复学习同一条query,就是我们以前可以看到那个macrosoft的这个py,然后以及一些其他的那些学习类型的一些小模型,然后LAMA好像也是这样做的,就是那些比如说课本的一些书籍,他们可能会去训练很多遍,但是这里的话它不是去让模型训练很多遍,而是说我不训练很多遍,而是说我对于这些很高质量的数据去做一轮改写,然后再去把它当做两条数据去做训练,那你这样其实也就起到了一个训练两遍的一个作用,然后它大体上就是想要提升这种高质量数据的一个占比,然后主要针对的是知识领域和数学领域,然后跟LAMA用book的数据去训练两遍其实也是一举同工的嘛,然后我们就来看一下它知识领域是怎么样去做的改写,然后它有三种方式,它有三种方式我这里写成ABC,第一个的话就是风格和视角的多样化的去做改写,那么你主要的话就是在保持事实情况一致的情况下,你去让它从不同的角度去给你写,然后不同的语言风格,这种的话都是通过写prompt的方式来实现的,然后第二个的话是通过分块做自回归的一种方式去对每一个段落进行一个改写,你先把完整的段落给到一个模型,然后把一小段落完整的input给到模型,然后以及把其中一小段给到模型,然后设计一个prompt,让它去针对这一小段进行改写,然后得到了之后呢,我再去改写另外一小段,然后慢慢的你就把整个文章全部改写完了,就是相当于你一段一段的去做改写,这个东西也很好在业务里面用起来,然后你这样改写出来其实也不一定是好的嘛,然后还有它会去做筛选嘛,那么你需要保证你改写出来的内容和原文的内容是一致的,所以说它还会去做一个保真度的一个教验,但是这个东西的话主要是做那个数据筛选用的,然后第二个部分就是数学领域的一个改写,那这个就相对来说比较有趣了,就不是说我们对于同一个事实的东西去做不同的风格,然后来保持它的事实完整性,而是说它是通过两种策略,一种是学习笔记的一种改写,就是相当于你这个知识点是很难的,但是我们人类去做笔记的时候,你可能会去用自己的话去解释一遍,然后把一些关键要点去写出来,然后再推导出一些答案,你可以看到这个我觉得这个策略还是相对来说比较聪明的,然后第二个的话就是去做那个多语言的一个翻译,但是这个的话在我印象里面以前应该也是有其他的一些团队在做这个事情的,相当于用那个多语言的语料去做训练,但是那个时候好像一般是为了获得模型的一个多语言的能力,而这里的话主要是为了提升这个数据的一个密度,高质量数据的一个密度,然后整体来说他通过这种改写的这种方式的话,就是能够在不增加过拟核的一个风险的情况下,提升了那个token的利用效率,就相当于你一条数据不用过两遍了,而是说你用改写前后的两条数据去做的训练,那么你这样过拟核的风险就相对来说比较少了,这就是那个预训练,然后他预训练成功主要就是两点,第一点的话就是那个moonclip的优化器,这个的话我们可能作为一个应用算法工程师,可能用的相对来说比较少,但是他的这种文本改写的一个策略对于我们来说还是比较有帮助的,因为我们很容易在自己的工作里面就用起来,尤其是做那些创意写作的一些同学,那么这个东西的话是相对来说比较有用的,接下来也就是本次视频的重点,对于算法工程师来说,尤其是应用算法工程师来说最有用的一个点就是后训练posted chaining,然后我个人觉得要比较关注的点是你的合成数据是怎么样做合成的,然后第二个的话是在没有rb reward的情况下,怎么样去训练一个通用的agentic的model,这是两个需要我们学习的,首先是第一个就是你合成数据是怎么合成的,其实我觉得这个东西好像也没有那么难想到,包括我自己以前在构造数据的时候,我其实也想过这么去做,但是又觉得它成本很高,总觉得那些预训练的一些公司可能会有一些其他的一些很好的一些方法去做的,但是从这个KimiK2的paper和报告来看,他们的那个方法也是去用真实的环境去做模拟,然后再去筛选,大致就是这样一个思路,我们可以先不看第一张图,我们先来看第二张图,第二张图的话我觉得会比较好解释一点,首先我们可以理解我们有很多很多的domain,就假设我们现在有一个domain叫做那个浏览器浏览相关的,那么我就可以去合成一些工具,然后我还可以从那个真实世界的mcp工具的一些库,比如说mcp点搜啊相关的一些网站,你就可以看到大家发布的各种各样的mcp工具,然后你就可以找到这一个domain的工具,然后把它配备给某一个agent去完成某一个特定的任务,去完成某一个特定的任务,这个task with rule barracks,意思就是可以被评估的一个一些任务,然后让某一个配备了工具的agent去完成,那么这个这个rule barracks,它其实就是LMS judge,也就是说你去提供一些评价标准,然后让模型针对于这个标准去打分,我个人觉得这个东西和以前的LMS judge没有什么区别啊,但是它是一个不知道为什么最近突然活起来的一个概念,大家如果有不同的意见也可以在评论区跟我交流一下,因为我对于这一块的理解可能不是特别深,我个人觉得它就是以前的LMS judge,包括我个人觉得我一看到这个东西的时候,我其实就想到了deepseq.grm这一篇文章,就是我们以前也录过视频,也写过文章,也阅读过他的那个文章,大概就是那个东西,然后我们假设已经得到了一个agent,并且是配备了工具的agent,然后并且这个任务也是可以被判断的,然后我们就去构建一个模拟的这个user的agent,也就是说让这一个useragent去提出一个问题,然后去跟你刚刚构造出来的那些agent,去让他去完成任务,你这个useragent主要是用来提出问题的,比如说你假设是一个浏览器相关的,你可能会告诉他你帮我订今天的机票,然后这个agent去做了什么什么东西之后,然后这个useragent可能要反映,他说,我觉得你订的不对,然后他又再去重新的去做一轮,大概就是这个意思,也就是说他去做用户的模拟,然后去做多轮的一个交互,然后再去把他的那个trajectory,也就是他这里面的一些交互的一些过程,给记录下来,得到一个trajectory,然后通过那个一个LM judge的方法去把这些数据给筛选出来,然后这个流程大家已经理解了吗,然后但是他事实上这些数据是可以进化的,就是这些数据是可以进化的,你就像刚刚你筛选数据,你可以得到第一轮的数据之后,你是可以去根据你的这一个标准完成的好坏,你可以去把他的数据质量提升到,你觉得没有办法提升了之后,然后再拿去把这个数据得到的,这也就是他这里的那个Evolve的一种体现,反正就是去大规模的去模拟,然后再去做筛选,我个人觉得这一张图可能就比较好理解,但是这个的话是我们需要考虑的,就是如果我们真的要在工作中训练了,训练自己的Agentic能力的话,那么你也得要用真实的MCP的工具,然后去去武装你的你的Agent,然后再去做训练,所以说通过这样一种方法的话,那么他就能够迅速的去扩展各种各样的领域,因为我刚说了这里是选择其中一个Domain嘛,那么你可以有很多的Domain,那么你根据这个方法的话,你就可以生产出很多很多的Agentic的一个数据,然后这个数据相对于我们传统的人去搞数据的话,他有一个好处就是他可以自动化的去合成,然后你就可以大规模的去扩量嘛,就比如说如果你发现了一个新的领域,或者说新的一些MCP工具,那么你就可以去构造这样一个Agent,然后用你的UserAgent去模拟,然后去筛选出来,筛选出来你想要的数据,你只要有了一个新的领域,有了新的MCP相关的一些工具,那么你就可以去筛选出你想要的Agentic的那个Tradectory,所以说这就是他怎么样去构造这个Agentic的训练数据的,这个的话相对来说,也很容易想到只是说我们要大胆,不要觉得这个东西成本很高,因为可能相对于预训练来说,这些成本可能不那么高呢,对不对,确实就是看完他之后,我就会觉得好像其实也没有那么高嘛,对吧,就是不要被自己的想象给束缚了,然后第二个的话就是在没有RawBase Reward的情况下,我们要怎么样去做通用的强化学习训练,那么你做强化学习训练你总归要有一个Reward嘛,那么他这里就是在不可验证的奖励的情况下去做训练,这里就是那个非RawBase的Reward,怎么样去做训练,然后他这里的思路其实也很简单,就是用另外一个LM,不对,用自己作为一个判别器,但是他也是一个LM,就LM去对这一次的输出进行一个打分,然后我们是不是就想到了这个DeepSecGRM这个模型,就是我们以前也讲过,就是你在对某一个Response做打分之前,你先输出这个Critic,然后再去输出他对应的那个分数,这个其实基本上就是一样的,但大家可以自己去参考一下,大概就是这个意思,你先输出你的Critic,然后再输出你的最终的那个Reward的值,然后来当做你训练的那个Reward,然后这里的话,这里的话是有一个老生常常的问题,就是在这里面的时候,其实我们也提到了,你用LM做Reward,他就准吗?到底和以前的LM as judge有什么区别呢?好像没有什么区别,对不对?其实他就是没有什么区别,他就是那一套,就是LM as judge做Reward的那一套,但是他这里的话是会自己先输出你的评估标准,然后再给出分数,并且是他自己,就是你训练中的那个模型去做打分,所以说他可以理解为是一种on policy的信号,然后第二个的话是,你这个模型是在慢慢的迭代的过程中吗?然后他是可以做改进的,然后他用的是,用那种可验证奖励的数据去改进你的critic,意思就是,你其实数据是有两部分的,一部分是可验证的,一部分是不可验证的,然后不可验证的这一部分我们是没有办法迭代的,没有办法去迭代他的critic部分,那么我就相当于我这个模型,只去迭代这部分可验证的,也就是说,看你输出的critic是不是能够推导出,他真正的那一个精确的reward,也就是说你输出的critic,慢慢的更贴近于他的真实的reward之后,那么你这个critic是不是就变得越来越好了,这就是这样一个思路,然后他通过这样一个大规模的合成数据之后,就可以看到k2在于工具调用的一些方面,他的能力已经变得非常的好了,所以说外网在7月份和9月份的时候,其实对那个k2的评价就很高了,那个openrouter啊,还有一些cursor啊都接入了,所以就可以看到其实k2的话,他还是做的相对来说比较不错的,然后接下来我们已经讲完了那个k2是怎么训练出来的,我们就来看一下k2 thinking,到底和这个k2有什么区别,我们其实前面也讲过,他主要就是有更强的工具调用能力,然后并且是增加了这个thinking的能力,然后通过这种test time scaling的方式去提升了这两方面的一个能力,这就是k2 thinking,然后我们可以再来回顾一下什么是这个test time scaling,他的核心意思就是你增加模型的推力时长,然后输出的token,那么他的最终的效果是会好的,其实就是那个kimi k1.5那篇paper提出来的嘛,我记得有一张图啊,有一张图,就是你的输出token越长,然后它的效果越好,就是这一个,就是这张图,就是你输出的token越长,然后它的效果越好,这就是所谓的test time scaling,然后同时你的训练数据中相对于k2的效果越好,相对于k2来说,他需要要有更强的这种调用工具的能力吗?有吗?就是调用工具的能力吗?那么他就会需要你模型先思考一段时间,然后调用工具,然后拿到工具的结果,然后再去做思考,那么这种方式是不是叫做interleaved reasoning,相当于你思考调用工具,思考调用工具,然后这个其实我们在那个k1.5这篇paper里面的时候,我也跟大家讲过了,图文交织的数据就是vlm的时候是怎么训练的,意思就是你写一段文本,然后配一张图,然后再写一段文本,然后再配一张图,这样模型就能更好的去理解图和文的关系,然后对于这个agentic的这种这种模型的话,那么他就会需要边思考边调用工具,然后来有这样子的训练数据,模型的能力才能提升,这也就是这种叫做interleaved reasoning的数据,然后这样子的话,模型才能够更加模拟它一个真实情况的,真实用户的一个使用习惯,然后最后我们来简要的看一下这个benchmark,因为这个benchmark其实不重要,我个人觉得,他说的这个比如说agentic search超过人类,这个是肯定的,我看openAI的那个01系列,他做那个deep research的时候,我觉得已经做的比我强太多了,就是现在我感觉我已经离不开deep research了,然后第二个我觉得比较重要的榜单,就是这个agentic的codein的能力,因为你其他的那些比如说含知识啊什么的,好像对于我们的生活也没有那么大的影响,因为他本身已经特别强了,然后我觉得大家可能会比较对标的一个模型就是cloud,那么你cloud主要就是你写代码能力很强,那么可以看到那种四维bench,那个kimi也已经追上来了,所以我觉得他相对来说还是比较不错的,所以才在外网上面受到比较多人的一个追捧吧,然后最后我们来对比一下他们俩的那个技术细节,就是k2和k2 thinking的那个技术细节,一个就是非思考模型,一个就是有思考的,就是具体指他会不会输出这种long-COT,然后第二个就是工具调用能力的知识,那个k2其实工具调用能力也还行,我看到很多那种写代码的一些产品都接入了这个k2,包括trade啊还有cursor都接入了,但是k2 thinking的话你看他可以两三百步调用,这个就真的很强啊,所以这种agentic的能力的话可能就是我们2025年agent元年最重要的一个能力了,然后比较适用的一个场景就是k2就是你快速响应嘛,然后k2 thinking的话,我个人觉得如果你不缺钱你就用k2 thinking呗,对不对,我是这么觉得,尽管他有一些那种过度生成和过度使用工具的一个现象,但是大部分情况下他是好的就行了嘛,我们就用好的就行了,如果不缺钱的话土豪就用这种模型就好了,然后最后回到作为一个算法工程师我们能学到什么,这里做一下总结啊,这个的话都是可以理解为是我的一个数学链,我个人觉得比较重要的一个点能在业务中用起来的就是agentic数据的一个改写,然后尤其是创意写作,而且他写得很详细,我们很容易就follow了,就像以前的那个怎么样去过滤掉prompt的难易程度,对吧,就是我们要做课程学习嘛,那么我们可能就要筛选掉一些简单的prompt,那么他写得很详细,然后第二个的话就是agentic数据构建的那个pipeline,以前大胆想象就是在真实环境下模拟,然后去用LM做筛选,我以前老是想着用另外一个大模型去去做生成,然后再迭代的去筛选,但实际上好像大家就是用真实的环境模拟,然后去做生成,然后第二个,然后第三个就是这个LM as judge,这个的话可能就比较考验大家写prompt的能力了,因为我总感觉这个好像一两年前大家就在用了,但不知道为什么又火了一遍,然后第四个就刚刚说的,你不缺钱的话就让模型输出的长一点吧,如果你的业务也不在乎延时的话,就让他输出的长一点,我觉得这个还是挺好的,然后最后再讲两点个人的碎碎念,就是我个人觉得从1.5开始,好像中国的Open source的模型越来越强了,包括最近的minimax的m2,然后glm的模型,然后kimi,像现在比较有名的那些模型,尤其是agentic的模型,好像都是国内的模型,拉玛已经没什么声音了,还有一个就是现在大家都在卷agentic的能力,都在对标这个cloud了,然后希望能把这个cloud的价格打下来吧,因为目前来说其实cloud在写代码上还是最强的,然后我是潮发,如果本期视频对你有帮助的话,欢迎大家一键三连,我给大家磕一个,磕疼了,然后如果想要获得更多的其他的一些文字相关的一些更新的话,可以关注我的个人公众号,也叫做潮发用代码打点加油,本次视频就到这里结束了,感谢大家。